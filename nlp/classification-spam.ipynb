{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier\n",
    "\n",
    "In this notebook, we will demostrate the end-to-end machine learning pipeline from data preparation to model selection for building a spam classifier for text messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read message file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thats the way u feel. Thats the way its gota b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ham</td>\n",
       "      <td>Is that seriously how you spell his name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>I‘m going to try for 2 months ha ha only joking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>So ü pay first lar... Then when is da stock comin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ffffffffff. Alright no way I can meet up with you sooner?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>Lol your always so convincing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ham</td>\n",
       "      <td>Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm back &amp;amp; we're packing the car now, I'll let you know if there's room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ahhh. Work. I vaguely remember that! What does it feel like? Lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wait that's still not all that clear, were you not sure about me being sarcastic or that that's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ham</td>\n",
       "      <td>K tell me anything about you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ham</td>\n",
       "      <td>For fear of fainting with the of all that housework you just did? Quick have a cuppa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spam</td>\n",
       "      <td>Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>ham</td>\n",
       "      <td>Armand says get your ass over to epsilon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>ham</td>\n",
       "      <td>U still havent got urself a jacket ah?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm taking derek &amp;amp; taylor to walmart, if I'm not back by the time you're done just leave the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hi its in durban are you still on this number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ic. There are a lotta childporn cars then.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Motorola, Nokia etc. all FREE! Double Mins &amp; Text on O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>ham</td>\n",
       "      <td>No, I was trying it all weekend ;V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>ham</td>\n",
       "      <td>You know, wot people wear. T shirts, jumpers, hat, belt, is all we know. We r at Cribbs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>ham</td>\n",
       "      <td>Cool, what time you think you can get here?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wen did you get so spiritual and deep. That's great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have a safe trip to Nigeria. Wish you happiness and very soon company to share moments with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hahaha..use your brain dear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>ham</td>\n",
       "      <td>Well keep in mind I've only got enough gas for one more round trip barring a sudden influx of cash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeh. Indians was nice. Tho it did kane me off a bit he he. We shud go out 4 a drink sometime soo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes i have. So that's why u texted. Pshew...missing you so much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>ham</td>\n",
       "      <td>No. I meant the calculation is the same. That  &amp;lt;#&amp;gt; units at  &amp;lt;#&amp;gt; . This school is re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5553</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>ham</td>\n",
       "      <td>if you aren't here in the next  &amp;lt;#&amp;gt;  hours imma flip my shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>ham</td>\n",
       "      <td>Anything lor. Juz both of us lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>ham</td>\n",
       "      <td>Get me out of this dump heap. My mom decided to come to lowes. BORING.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lor... Sony ericsson salesman... I ask shuhui then she say quite gd 2 use so i considering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>ham</td>\n",
       "      <td>Why don't you wait 'til at least wednesday to see if you get your .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>ham</td>\n",
       "      <td>Huh y lei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5567 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  \\\n",
       "0     spam   \n",
       "1      ham   \n",
       "2      ham   \n",
       "3      ham   \n",
       "4      ham   \n",
       "5     spam   \n",
       "6     spam   \n",
       "7      ham   \n",
       "8     spam   \n",
       "9     spam   \n",
       "10    spam   \n",
       "11     ham   \n",
       "12     ham   \n",
       "13     ham   \n",
       "14    spam   \n",
       "15     ham   \n",
       "16     ham   \n",
       "17     ham   \n",
       "18     ham   \n",
       "19     ham   \n",
       "20     ham   \n",
       "21     ham   \n",
       "22     ham   \n",
       "23     ham   \n",
       "24     ham   \n",
       "25     ham   \n",
       "26     ham   \n",
       "27     ham   \n",
       "28     ham   \n",
       "29    spam   \n",
       "...    ...   \n",
       "5537   ham   \n",
       "5538   ham   \n",
       "5539   ham   \n",
       "5540   ham   \n",
       "5541   ham   \n",
       "5542  spam   \n",
       "5543   ham   \n",
       "5544   ham   \n",
       "5545   ham   \n",
       "5546   ham   \n",
       "5547   ham   \n",
       "5548   ham   \n",
       "5549   ham   \n",
       "5550   ham   \n",
       "5551   ham   \n",
       "5552   ham   \n",
       "5553   ham   \n",
       "5554   ham   \n",
       "5555   ham   \n",
       "5556   ham   \n",
       "5557   ham   \n",
       "5558   ham   \n",
       "5559   ham   \n",
       "5560   ham   \n",
       "5561  spam   \n",
       "5562  spam   \n",
       "5563   ham   \n",
       "5564   ham   \n",
       "5565   ham   \n",
       "5566   ham   \n",
       "\n",
       "                                                                                                body_text  \n",
       "0     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                           Nah I don't think he goes to usf, he lives around here though  \n",
       "2                           Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                     I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4     As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  \n",
       "5     WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c...  \n",
       "6     Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...  \n",
       "7     I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...  \n",
       "8     SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, ...  \n",
       "9     URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM...  \n",
       "10    XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here...  \n",
       "11                                                                             Oh k...i'm watching here:)  \n",
       "12                      Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.  \n",
       "13                                               Fine if thats the way u feel. Thats the way its gota b  \n",
       "14    England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to...  \n",
       "15                                                              Is that seriously how you spell his name?  \n",
       "16                                                        I‘m going to try for 2 months ha ha only joking  \n",
       "17                                                   So ü pay first lar... Then when is da stock comin...  \n",
       "18               Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?  \n",
       "19                                              Ffffffffff. Alright no way I can meet up with you sooner?  \n",
       "20    Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worrie...  \n",
       "21                                                                         Lol your always so convincing.  \n",
       "22    Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's le...  \n",
       "23                            I'm back &amp; we're packing the car now, I'll let you know if there's room  \n",
       "24                                       Ahhh. Work. I vaguely remember that! What does it feel like? Lol  \n",
       "25    Wait that's still not all that clear, were you not sure about me being sarcastic or that that's ...  \n",
       "26    Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child a...  \n",
       "27                                                                          K tell me anything about you.  \n",
       "28                   For fear of fainting with the of all that housework you just did? Quick have a cuppa  \n",
       "29    Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm ...  \n",
       "...                                                                                                   ...  \n",
       "5537                                                             Armand says get your ass over to epsilon  \n",
       "5538                                                               U still havent got urself a jacket ah?  \n",
       "5539  I'm taking derek &amp; taylor to walmart, if I'm not back by the time you're done just leave the...  \n",
       "5540                                                        Hi its in durban are you still on this number  \n",
       "5541                                                           Ic. There are a lotta childporn cars then.  \n",
       "5542  Had your contract mobile 11 Mnths? Latest Motorola, Nokia etc. all FREE! Double Mins & Text on O...  \n",
       "5543                                                                   No, I was trying it all weekend ;V  \n",
       "5544              You know, wot people wear. T shirts, jumpers, hat, belt, is all we know. We r at Cribbs  \n",
       "5545                                                          Cool, what time you think you can get here?  \n",
       "5546                                                  Wen did you get so spiritual and deep. That's great  \n",
       "5547          Have a safe trip to Nigeria. Wish you happiness and very soon company to share moments with  \n",
       "5548                                                                          Hahaha..use your brain dear  \n",
       "5549   Well keep in mind I've only got enough gas for one more round trip barring a sudden influx of cash  \n",
       "5550  Yeh. Indians was nice. Tho it did kane me off a bit he he. We shud go out 4 a drink sometime soo...  \n",
       "5551                                      Yes i have. So that's why u texted. Pshew...missing you so much  \n",
       "5552  No. I meant the calculation is the same. That  &lt;#&gt; units at  &lt;#&gt; . This school is re...  \n",
       "5553                                                                               Sorry, I'll call later  \n",
       "5554                                   if you aren't here in the next  &lt;#&gt;  hours imma flip my shit  \n",
       "5555                                                                    Anything lor. Juz both of us lor.  \n",
       "5556                               Get me out of this dump heap. My mom decided to come to lowes. BORING.  \n",
       "5557     Ok lor... Sony ericsson salesman... I ask shuhui then she say quite gd 2 use so i considering...  \n",
       "5558                                                                                  Ard 6 like dat lor.  \n",
       "5559                                  Why don't you wait 'til at least wednesday to see if you get your .  \n",
       "5560                                                                                         Huh y lei...  \n",
       "5561  REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 th...  \n",
       "5562  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy...  \n",
       "5563                                                                 Will ü b going to esplanade fr home?  \n",
       "5564                                            Pity, * was in mood for that. So...any other suggestions?  \n",
       "5565  The guy did some bitching but I acted like i'd be interested in buying something else next week ...  \n",
       "5566                                                                           Rofl. Its true to its name  \n",
       "\n",
       "[5567 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Stemming: process of reducing inflected (or sometimes derived) words to their word stem or root\n",
    "\n",
    "Lemmatization: process of grouping together the inflected forms of a word so they can be analytzed as a single term, identified by the word's lemma\n",
    "\n",
    "Example:\n",
    "\n",
    "| **Words** | **Stemming** | **Lemmatization** |\n",
    "|-----------|--------------|-------------------|\n",
    "| mean      | mean         | mean              |\n",
    "| means     | mean         | mean              |\n",
    "| meaning   | mean         | meaning           |\n",
    "| meanness  | mean         | meanness          |\n",
    "| goose     | goos         | goose             |\n",
    "| geese     | gees         | goose             |\n",
    "\n",
    "### Tradeoffs between lemmatization and stemming\n",
    "\n",
    "The goal of lemmatization and stemming is to condense derived words into their base forms. However, the 2 approaches have tradeoffs between accuracy and speed. \n",
    "\n",
    "Stemming is typically **faster** but less **accurate** as it simply chops off the end of a word using heuristics, with understanding of the context in which a word is used. \n",
    "\n",
    "Lemmatizing is typically more **accurate** but **slower** as it uses vocabulary analysis to reduce groups of words with similar meaning to the dictionary form of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove punctuations & lowercase text\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "    # 3.a. Remove stopwords & stemming\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    \n",
    "#     # 3.b. Remove stopwords & lemmatization\n",
    "#     text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Create new features or transform existing features to add dimensions to the feature set. In this case, we are creating 2 new features: length of text and punctuation percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage of punctuations within a string\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "# Create new features: length of text and punctuation percentage\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "Split data into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['body_text', 'body_len', 'punct%']],\n",
    "    data['label'],\n",
    "    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Vectorization is the proess of encoding text as integers to create feature vectors. There are many text vectorization techniques. [Count vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), [n-gram vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer) (see `ngram_range`), and [term frequency-inverse document frequency (TF-IDF) weighting](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) are examples of such techniques.\n",
    "\n",
    "- Count vectorization: creates a document-term matrix where the entry of each cell will be a count of the number of times that word occurred in that document\n",
    "\n",
    "  - Example:\n",
    "\n",
    "![vectorization_example.jpg](./doc/vectorization_example.jpg)\n",
    "\n",
    "- N-gram vectorization: creates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combinations of adjacent words of length n in your text.\n",
    "\n",
    "  - Example: \"NLP is an interesting topic\"\n",
    "\n",
    "| n | Name      | Tokens                                                         |\n",
    "|---|-----------|----------------------------------------------------------------|\n",
    "| 2 | bigram    | [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"]      |\n",
    "| 3 | trigram   | [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] |\n",
    "| 4 | four-gram | [\"nlp is an interesting\", \"is an interesting topic\"]    |\n",
    "\n",
    "- Term frequency-inverse document frequency (TF-IDF): Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document.\n",
    "\n",
    "![tf-idf.jpg](./doc/tf-idf.jpg)\n",
    "\n",
    "For this notebook, we will be using TF-IDF weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>7206</th>\n",
       "      <th>7207</th>\n",
       "      <th>7208</th>\n",
       "      <th>7209</th>\n",
       "      <th>7210</th>\n",
       "      <th>7211</th>\n",
       "      <th>7212</th>\n",
       "      <th>7213</th>\n",
       "      <th>7214</th>\n",
       "      <th>7215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...   7206  7207  \\\n",
       "0        41     2.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "1        72     2.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "2       123     5.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "3        21     9.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "4       124     6.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "\n",
       "   7208  7209  7210  7211  7212  7213  7214  7215  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 7218 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "                         pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "                        pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Random Forest: ensemble learning method that constructs a collection of decision tree and then aggregates the predictions of each tree to determine the final prediction (bagging)\n",
    "\n",
    "Gradient Boosting: ensemble learning method that takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes of prior iterations (boosting)\n",
    "\n",
    "### Comparison of Random Forest and Gradient Boosting\n",
    "\n",
    "![tradeoff_randomforest_gradientboosting.jpg](./doc/tradeoff_randomforest_gradientboosting.jpg)\n",
    "\n",
    "In this notebook, we will train both random forest and gradient boosting models for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.866139</td>\n",
       "      <td>0.262468</td>\n",
       "      <td>0.199870</td>\n",
       "      <td>0.009523</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.971942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972378</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.584056</td>\n",
       "      <td>0.441370</td>\n",
       "      <td>0.190365</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.968575</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.971942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.026667</td>\n",
       "      <td>0.070631</td>\n",
       "      <td>0.122778</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.968575</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.971942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971704</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.647201</td>\n",
       "      <td>0.254553</td>\n",
       "      <td>0.121825</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971480</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.627550</td>\n",
       "      <td>0.172325</td>\n",
       "      <td>0.101679</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 150}</td>\n",
       "      <td>0.964085</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.973064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971031</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>5</td>\n",
       "      <td>0.995508</td>\n",
       "      <td>0.994385</td>\n",
       "      <td>0.994947</td>\n",
       "      <td>0.993825</td>\n",
       "      <td>0.994106</td>\n",
       "      <td>0.994554</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8       15.866139      0.262468         0.199870        0.009523   \n",
       "11      16.584056      0.441370         0.190365        0.008809   \n",
       "7        8.026667      0.070631         0.122778        0.010428   \n",
       "10       8.647201      0.254553         0.121825        0.003741   \n",
       "4        6.627550      0.172325         0.101679        0.004706   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "8               90                300   \n",
       "11            None                300   \n",
       "7               90                150   \n",
       "10            None                150   \n",
       "4               60                150   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.967452   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.968575   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.968575   \n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.969697   \n",
       "4     {'max_depth': 60, 'n_estimators': 150}           0.964085   \n",
       "\n",
       "    split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "8            0.970819           0.971942       ...                0.972378   \n",
       "11           0.970819           0.971942       ...                0.972154   \n",
       "7            0.970819           0.971942       ...                0.971704   \n",
       "10           0.970819           0.969697       ...                0.971480   \n",
       "4            0.969697           0.973064       ...                0.971031   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "8         0.004516                1            0.999439            0.999439   \n",
       "11        0.003972                2            1.000000            1.000000   \n",
       "7         0.004339                3            0.999158            0.999439   \n",
       "10        0.003055                4            1.000000            1.000000   \n",
       "4         0.004777                5            0.995508            0.994385   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "8             0.999158            0.999439            0.999439   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "7             0.999439            0.999439            0.999439   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "4             0.994947            0.993825            0.994106   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "8           0.999382         0.000112  \n",
       "11          1.000000         0.000000  \n",
       "7           0.999382         0.000112  \n",
       "10          1.000000         0.000000  \n",
       "4           0.994554         0.000604  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gridsearch for random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "param = {\n",
    "    'n_estimators': [10, 150, 300],\n",
    "    'max_depth': [30, 60, 90, None]\n",
    "}\n",
    "\n",
    "rfclf = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "rfclf_fit = rfclf.fit(X_train_vect, y_train)\n",
    "\n",
    "df_rfclf_cv_results = pd.DataFrame(rfclf_fit.cv_results_) \\\n",
    "    .sort_values('mean_test_score', ascending=False)[0:5]\n",
    "df_rfclf_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch for gradient boosting classifier\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "gbclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "gbclf_fit = gbclf.fit(X_train_vect, y_train)\n",
    "\n",
    "df_gbclf_cv_results = pd.DataFrame(gbclf_fit.cv_results_) \\\n",
    "    .sort_values('mean_test_score', ascending=False)[0:5]\n",
    "df_gbclf_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Evaluate best random forest and gradient boosting hyperparameters on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 90, 'n_estimators': 300}\n",
      "Fit time: 5.311 / Predict time: 0.192 ---- Precision: 1.0 / Recall: 0.824 / Accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "best_rfclf_hyperparam = df_rfclf_cv_results.head(1)['params'].values[0]\n",
    "print(best_rfclf_hyperparam)\n",
    "\n",
    "rf = RandomForestClassifier(**best_rfclf_hyperparam, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3),\n",
    "    round(pred_time, 3),\n",
    "    round(precision, 3),\n",
    "    round(recall, 3),\n",
    "    round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 150}\n",
      "Fit time: 240.042 / Predict time: 0.084 ---- Precision: 0.942 / Recall: 0.85 / Accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "best_gbclf_hyperparam = df_gbclf_cv_results.head(1)['params'].values[0]\n",
    "print(best_gbclf_hyperparam)\n",
    "\n",
    "gb = GradientBoostingClassifier(**best_gbclf_hyperparam)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3),\n",
    "    round(pred_time, 3),\n",
    "    round(precision, 3),\n",
    "    round(recall, 3),\n",
    "    round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
